"""TODO: change this as MNIST"""
import tensorflow as tf

### mask_effective_attack is to be implemented
class ModelDrebin(object):
  def __init__(self, precision=tf.float32):
      self.precision = precision

  def _encoder(self, x_input, y_input, is_train, mask_effective_attack=False):
      with tf.variable_scope('main_encoder', reuse=tf.AUTO_REUSE):
        self.x_input = x_input
        self.y_input = y_input
        self.is_training = is_train

        # self.x_input = tf.reshape(self.x_input, [-1, 545334])

        # first fully connected layer
        W_fc1 = self._weight_variable([545334, 200], scope='fcw1')
        b_fc1 = self._bias_variable([200], scope='fcb1')
        self.x3 = tf.nn.relu(tf.matmul(self.x_input, W_fc1) + b_fc1)

        # second fully connected layer
        W_fc2 = self._weight_variable([200, 200], scope='fcw2')
        b_fc2 = self._bias_variable([200], scope='fcb2')
        self.x4 = tf.nn.relu(tf.matmul(self.x3, W_fc2) + b_fc2)

        # output layer
        W_fc3 = self._weight_variable([200, 2], scope='fcw3')
        b_fc3 = self._bias_variable([2], scope='fcb3')
        self.pre_softmax = tf.matmul(self.x4, W_fc3) + b_fc3

        y_xent = tf.nn.sparse_softmax_cross_entropy_with_logits(
            labels=self.y_input, logits=self.pre_softmax)

        self.y_xent = y_xent
        self.xent = tf.reduce_sum(y_xent)

        # Replace reduce_mean with matmul
        # self.mean_xent = tf.reduce_mean(y_xent)
        self.mean_xent = self.reduce_sum_det(y_xent) / tf.cast(tf.shape(y_xent)[0], dtype=self.precision)

        self.y_pred = tf.argmax(self.pre_softmax, 1)

        self.weight_decay_loss = self._decay()

        self.correct_prediction = tf.equal(self.y_pred, self.y_input)

        mask = tf.cast(self.correct_prediction, tf.int64)

        self.num_correct = tf.reduce_sum(tf.cast(self.correct_prediction, tf.int64))
        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))

        x0 = None
        x1 = None
        x2 = None
        x3 = self.x3
        x4 = self.x4
        pre_softmax = self.pre_softmax
        xent = self.xent
        mean_xent = self.mean_xent
        weight_decay_loss = self.weight_decay_loss
        num_correct = self.num_correct
        accuracy = self.accuracy
        predictions = self.y_pred
        mask = mask

        layer_values = {'x0':x0, 'x1':x1, 'x2':x2, 'x3':x3, 'x4':x4, 'pre_softmax':pre_softmax}

        return [layer_values, xent, mean_xent, weight_decay_loss, num_correct, accuracy, predictions, mask]


  def reduce_sum_det(self, x):
      v = tf.reshape(x, [1, -1])
      return tf.reshape(tf.matmul(v, tf.ones_like(v, dtype=self.precision), transpose_b=True), [])

  def _weight_variable(self, shape, scope):
      with tf.variable_scope(scope):
          w = tf.get_variable('DW', dtype=self.precision, initializer=tf.truncated_normal(shape, stddev=0.1, dtype=self.precision))  #TODO: init is a constant
      return w

  def _bias_variable(self, out_dim, scope):
      with tf.variable_scope(scope):
        b = tf.get_variable('biases', dtype=self.precision,
                          initializer= tf.constant(0.1, shape = [out_dim[0]], dtype=self.precision))
      return b

  def _decay(self):
    """L2 weight decay loss."""
    costs = []
    for var in tf.trainable_variables():
      if var.op.name.find('DW') > 0:
        costs.append(tf.nn.l2_loss(var))
    return tf.add_n(costs)
